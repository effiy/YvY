# 原始URL: https://github.com/elicit/machine-learning-list

# 抓取时间: 2025-03-30 21:17:32

[Skip to content](https://github.com/elicit/machine-learning-list#start-of-content)
## Navigation Menu
Toggle navigation
[ ](https://github.com/)
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Felicit%2Fmachine-learning-list)
  * Product 
    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)
    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)
    * [ Actions Automate any workflow  ](https://github.com/features/actions)
    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)
    * [ Issues Plan and track work  ](https://github.com/features/issues)
    * [ Code Review Manage code changes  ](https://github.com/features/code-review)
    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)
    * [ Code Search Find more, search less  ](https://github.com/features/code-search)
Explore
    * [ All features ](https://github.com/features)
    * [ Documentation ](https://docs.github.com)
    * [ GitHub Skills ](https://skills.github.com)
    * [ Blog ](https://github.blog)
  * Solutions 
By company size
    * [ Enterprises ](https://github.com/enterprise)
    * [ Small and medium teams ](https://github.com/team)
    * [ Startups ](https://github.com/enterprise/startups)
    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)
By use case
    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)
    * [ DevOps ](https://github.com/solutions/use-case/devops)
    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)
    * [ View all use cases ](https://github.com/solutions/use-case)
By industry
    * [ Healthcare ](https://github.com/solutions/industry/healthcare)
    * [ Financial services ](https://github.com/solutions/industry/financial-services)
    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)
    * [ Government ](https://github.com/solutions/industry/government)
    * [ View all industries ](https://github.com/solutions/industry)
[ View all solutions ](https://github.com/solutions)
  * Resources 
Topics
    * [ AI ](https://github.com/resources/articles/ai)
    * [ DevOps ](https://github.com/resources/articles/devops)
    * [ Security ](https://github.com/resources/articles/security)
    * [ Software Development ](https://github.com/resources/articles/software-development)
    * [ View all ](https://github.com/resources/articles)
Explore
    * [ Learning Pathways ](https://resources.github.com/learn/pathways)
    * [ Events & Webinars ](https://resources.github.com)
    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)
    * [ Customer Stories ](https://github.com/customer-stories)
    * [ Partners ](https://partner.github.com)
    * [ Executive Insights ](https://github.com/solutions/executive-insights)
  * Open Source 
    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)
    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)
Repositories
    * [ Topics ](https://github.com/topics)
    * [ Trending ](https://github.com/trending)
    * [ Collections ](https://github.com/collections)
  * Enterprise 
    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)
Available add-ons
    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)
    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)
    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)
  * [Pricing](https://github.com/pricing)


Search or jump to...
# Search code, repositories, users, issues, pull requests...
Search 
Clear
[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)
#  Provide feedback 
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel  Submit feedback 
#  Saved searches 
## Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). 
Cancel  Create saved search 
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Felicit%2Fmachine-learning-list)
[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=elicit%2Fmachine-learning-list) Reseting focus
You signed in with another tab or window. [Reload](https://github.com/elicit/machine-learning-list) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/elicit/machine-learning-list) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/elicit/machine-learning-list) to refresh your session. Dismiss alert
{{ message }}
[ elicit ](https://github.com/elicit) / **[machine-learning-list](https://github.com/elicit/machine-learning-list) ** Public
  * [ Notifications ](https://github.com/login?return_to=%2Felicit%2Fmachine-learning-list) You must be signed in to change notification settings
  * [ Fork 104 ](https://github.com/login?return_to=%2Felicit%2Fmachine-learning-list)
  * [ Star  1.2k ](https://github.com/login?return_to=%2Felicit%2Fmachine-learning-list)


A curriculum for learning about foundation models, from scratch to the frontier 
[elicit.com/careers](https://elicit.com/careers "https://elicit.com/careers")
[ 1.2k stars ](https://github.com/elicit/machine-learning-list/stargazers) [ 104 forks ](https://github.com/elicit/machine-learning-list/forks) [ Branches ](https://github.com/elicit/machine-learning-list/branches) [ Tags ](https://github.com/elicit/machine-learning-list/tags) [ Activity ](https://github.com/elicit/machine-learning-list/activity)
[ Star  ](https://github.com/login?return_to=%2Felicit%2Fmachine-learning-list)
[ Notifications ](https://github.com/login?return_to=%2Felicit%2Fmachine-learning-list) You must be signed in to change notification settings
  * [ Code ](https://github.com/elicit/machine-learning-list)
  * [ Issues 0 ](https://github.com/elicit/machine-learning-list/issues)
  * [ Pull requests 1 ](https://github.com/elicit/machine-learning-list/pulls)
  * [ Actions ](https://github.com/elicit/machine-learning-list/actions)
  * [ Security ](https://github.com/elicit/machine-learning-list/security)
  * [ Insights ](https://github.com/elicit/machine-learning-list/pulse)


Additional navigation options
  * [ Code  ](https://github.com/elicit/machine-learning-list)
  * [ Issues  ](https://github.com/elicit/machine-learning-list/issues)
  * [ Pull requests  ](https://github.com/elicit/machine-learning-list/pulls)
  * [ Actions  ](https://github.com/elicit/machine-learning-list/actions)
  * [ Security  ](https://github.com/elicit/machine-learning-list/security)
  * [ Insights  ](https://github.com/elicit/machine-learning-list/pulse)


# elicit/machine-learning-list
main
[Branches](https://github.com/elicit/machine-learning-list/branches)[Tags](https://github.com/elicit/machine-learning-list/tags)
[](https://github.com/elicit/machine-learning-list/branches)[](https://github.com/elicit/machine-learning-list/tags)
Go to file
Code
## Folders and files
Name| Name| Last commit message| Last commit date  
---|---|---|---  
## Latest commit
## History
[22 Commits](https://github.com/elicit/machine-learning-list/commits/main/)[](https://github.com/elicit/machine-learning-list/commits/main/)  
[README.md](https://github.com/elicit/machine-learning-list/blob/main/README.md "README.md")| [README.md](https://github.com/elicit/machine-learning-list/blob/main/README.md "README.md")  
View all files  
## Repository files navigation
  * [README](https://github.com/elicit/machine-learning-list)


# Elicit Machine Learning Reading List
[](https://github.com/elicit/machine-learning-list#elicit-machine-learning-reading-list)
## Purpose
[](https://github.com/elicit/machine-learning-list#purpose)
The purpose of this curriculum is to help new [Elicit](https://elicit.com/) employees learn background in machine learning, with a focus on language models. I’ve tried to strike a balance between papers that are relevant for deploying ML in production and techniques that matter for longer-term scalability.
If you don’t work at Elicit yet - we’re [hiring ML and software engineers](https://elicit.com/careers).
## How to read
[](https://github.com/elicit/machine-learning-list#how-to-read)
Recommended reading order:
  1. Read “Tier 1” for all topics
  2. Read “Tier 2” for all topics
  3. Etc


✨ Added after 2024/4/1
## Table of contents
[](https://github.com/elicit/machine-learning-list#table-of-contents)
  * [Fundamentals](https://github.com/elicit/machine-learning-list#fundamentals)
    * [Introduction to machine learning](https://github.com/elicit/machine-learning-list#introduction-to-machine-learning)
    * [Transformers](https://github.com/elicit/machine-learning-list#transformers)
    * [Key foundation model architectures](https://github.com/elicit/machine-learning-list#key-foundation-model-architectures)
    * [Training and finetuning](https://github.com/elicit/machine-learning-list#training-and-finetuning)
  * [Reasoning and runtime strategies](https://github.com/elicit/machine-learning-list#reasoning-and-runtime-strategies)
    * [In-context reasoning](https://github.com/elicit/machine-learning-list#in-context-reasoning)
    * [Task decomposition](https://github.com/elicit/machine-learning-list#task-decomposition)
    * [Debate](https://github.com/elicit/machine-learning-list#debate)
    * [Tool use and scaffolding](https://github.com/elicit/machine-learning-list#tool-use-and-scaffolding)
    * [Honesty, factuality, and epistemics](https://github.com/elicit/machine-learning-list#honesty-factuality-and-epistemics)
  * [Applications](https://github.com/elicit/machine-learning-list#applications)
    * [Science](https://github.com/elicit/machine-learning-list#science)
    * [Forecasting](https://github.com/elicit/machine-learning-list#forecasting)
    * [Search and ranking](https://github.com/elicit/machine-learning-list#search-and-ranking)
  * [ML in practice](https://github.com/elicit/machine-learning-list#ml-in-practice)
    * [Production deployment](https://github.com/elicit/machine-learning-list#production-deployment)
    * [Benchmarks](https://github.com/elicit/machine-learning-list#benchmarks)
    * [Datasets](https://github.com/elicit/machine-learning-list#datasets)
  * [Advanced topics](https://github.com/elicit/machine-learning-list#advanced-topics)
    * [World models and causality](https://github.com/elicit/machine-learning-list#world-models-and-causality)
    * [Planning](https://github.com/elicit/machine-learning-list#planning)
    * [Uncertainty, calibration, and active learning](https://github.com/elicit/machine-learning-list#uncertainty-calibration-and-active-learning)
    * [Interpretability and model editing](https://github.com/elicit/machine-learning-list#interpretability-and-model-editing)
    * [Reinforcement learning](https://github.com/elicit/machine-learning-list#reinforcement-learning)
  * [The big picture](https://github.com/elicit/machine-learning-list#the-big-picture)
    * [AI scaling](https://github.com/elicit/machine-learning-list#ai-scaling)
    * [AI safety](https://github.com/elicit/machine-learning-list#ai-safety)
    * [Economic and social impacts](https://github.com/elicit/machine-learning-list#economic-and-social-impacts)
    * [Philosophy](https://github.com/elicit/machine-learning-list#philosophy)
  * [Maintainer](https://github.com/elicit/machine-learning-list#maintainer)


## Fundamentals
[](https://github.com/elicit/machine-learning-list#fundamentals)
### Introduction to machine learning
[](https://github.com/elicit/machine-learning-list#introduction-to-machine-learning)
**Tier 1**
  * [A short introduction to machine learning](https://www.alignmentforum.org/posts/qE73pqxAZmeACsAdF/a-short-introduction-to-machine-learning)
  * [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk&t=0s)
  * [Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w)


**Tier 2**
  * ✨ [An intuitive understanding of backpropagation](https://cs231n.github.io/optimization-2/)
  * [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)
  * [An introduction to deep reinforcement learning](https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c)


**Tier 3**
  * [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)
  * [Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5)


### Transformers
[](https://github.com/elicit/machine-learning-list#transformers)
**Tier 1**
  * ✨ [But what is a GPT? Visual intro to transformers](https://www.youtube.com/watch?v=wjZofJX0v4M)
  * ✨ [Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc)
  * ✨ [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)
  * [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
  * [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)


**Tier 2**
  * ✨ [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
  * ✨ [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)
  * [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
  * [Attention Is All You Need](https://arxiv.org/abs/1706.03762)


**Tier 3**
  * [A Practical Survey on Faster and Lighter Transformers](https://arxiv.org/abs/2103.14636)
  * [TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second](https://arxiv.org/abs/2207.01848)
  * [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)
  * [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)

**Tier 4+**
  * ✨ [Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks](https://arxiv.org/abs/2311.12997)
  * [Memorizing Transformers](https://arxiv.org/abs/2203.08913)
  * [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)


### Key foundation model architectures
[](https://github.com/elicit/machine-learning-list#key-foundation-model-architectures)
**Tier 1**
  * [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe) (GPT-2)
  * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)


**Tier 2**
  * ✨ [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/abs/2302.13971) (LLaMA)
  * ✨ [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396) ([video](https://www.youtube.com/watch?v=EvQ3ncuriCM)) (S4)
  * [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (T5)
  * [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374) (OpenAI Codex)
  * [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (OpenAI Instruct)


**Tier 3**
  * ✨ [Mistral 7B](http://arxiv.org/abs/2310.06825) (Mistral)
  * ✨ [Mixtral of Experts](http://arxiv.org/abs/2401.04088) (Mixtral)
  * ✨ [Gemini: A Family of Highly Capable Multimodal Models](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) (Gemini)
  * ✨ [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752v1) (Mamba)
  * [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) (Flan)

**Tier 4+**
  * ✨ [Consistency Models](http://arxiv.org/abs/2303.01469)
  * ✨ [Model Card and Evaluations for Claude Models](https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf) (Claude 2)
  * ✨ [OLMo: Accelerating the Science of Language Models](http://arxiv.org/abs/2402.00838)
  * ✨ [PaLM 2 Technical Report](https://arxiv.org/abs/2305.10403) (Palm 2)
  * ✨ [Textbooks Are All You Need II: phi-1.5 technical report](http://arxiv.org/abs/2309.05463) (phi 1.5)
  * ✨ [Visual Instruction Tuning](http://arxiv.org/abs/2304.08485) (LLaVA)
  * [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/abs/2112.00861)
  * [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652) (Google Instruct)
  * [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
  * [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239) (Google Dialog)
  * [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2112.11446) (Meta GPT-3)
  * [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) (PaLM)
  * [Program Synthesis with Large Language Models](https://arxiv.org/abs/2108.07732) (Google Codex)
  * [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446) (Gopher)
  * [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858) (Minerva)
  * [UL2: Unifying Language Learning Paradigms](http://aima.cs.berkeley.edu/) (UL2)


### Training and finetuning
[](https://github.com/elicit/machine-learning-list#training-and-finetuning)
**Tier 2**
  * ✨ [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)
  * [Learning to summarise with human feedback](https://arxiv.org/abs/2009.01325)
  * [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)


**Tier 3**
  * ✨ [Pretraining Language Models with Human Preferences](http://arxiv.org/abs/2302.08582)
  * ✨ [Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](http://arxiv.org/abs/2312.09390)
  * [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638v1)
  * [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
  * [Unsupervised Neural Machine Translation with Generative Language Models Only](https://arxiv.org/abs/2110.05448)

**Tier 4+**
  * ✨ [Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](http://arxiv.org/abs/2312.06585)
  * ✨ [Improving Code Generation by Training with Natural Language Feedback](http://arxiv.org/abs/2303.16749)
  * ✨ [Language Modeling Is Compression](https://arxiv.org/abs/2309.10668v1)
  * ✨ [LIMA: Less Is More for Alignment](http://arxiv.org/abs/2305.11206)
  * ✨ [Learning to Compress Prompts with Gist Tokens](http://arxiv.org/abs/2304.08467)
  * ✨ [Lost in the Middle: How Language Models Use Long Contexts](http://arxiv.org/abs/2307.03172)
  * ✨ [QLoRA: Efficient Finetuning of Quantized LLMs](http://arxiv.org/abs/2305.14314)
  * ✨ [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](http://arxiv.org/abs/2403.09629)
  * ✨ [Reinforced Self-Training (ReST) for Language Modeling](http://arxiv.org/abs/2308.08998)
  * ✨ [Solving olympiad geometry without human demonstrations](https://www.nature.com/articles/s41586-023-06747-5)
  * ✨ [Tell, don't show: Declarative facts influence how LLMs generalize](http://arxiv.org/abs/2312.07779)
  * ✨ [Textbooks Are All You Need](http://arxiv.org/abs/2306.11644)
  * ✨ [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](http://arxiv.org/abs/2305.07759)
  * ✨ [Training Language Models with Language Feedback at Scale](http://arxiv.org/abs/2303.16755)
  * ✨ [Turing Complete Transformers: Two Transformers Are More Powerful Than One](https://openreview.net/forum?id=MGWsPGogLH)
  * [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
  * [Data Distributional Properties Drive Emergent In-Context Learning in Transformers](https://arxiv.org/abs/2205.05055)
  * [Diffusion-LM Improves Controllable Text Generation](https://arxiv.org/abs/2205.14217)
  * [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137)
  * [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)
  * [ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](https://arxiv.org/abs/2111.10952)
  * [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
  * [Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning](https://arxiv.org/abs/2106.02584)
  * [True Few-Shot Learning with Prompts -- A Real-World Perspective](https://arxiv.org/abs/2111.13440)


## Reasoning and runtime strategies
[](https://github.com/elicit/machine-learning-list#reasoning-and-runtime-strategies)
### In-context reasoning
[](https://github.com/elicit/machine-learning-list#in-context-reasoning)
**Tier 2**
  * [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
  * [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) (Let's think step by step)
  * [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)


**Tier 3**
  * ✨ [Chain-of-Thought Reasoning Without Prompting](http://arxiv.org/abs/2402.10200)
  * ✨ [Why think step-by-step? Reasoning emerges from the locality of experience](http://arxiv.org/abs/2304.03843)

**Tier 4+**
  * ✨ [Baldur: Whole-Proof Generation and Repair with Large Language Models](https://arxiv.org/abs/2303.04910v1)
  * ✨ [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](http://arxiv.org/abs/2403.05518)
  * ✨ [Certified Reasoning with Language Models](http://arxiv.org/abs/2306.04031)
  * ✨ [Hypothesis Search: Inductive Reasoning with Language Models](http://arxiv.org/abs/2309.05660)
  * ✨ [LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations](http://arxiv.org/abs/2305.18354)
  * ✨ [Large Language Models Cannot Self-Correct Reasoning Yet](https://arxiv.org/abs/2310.01798v1)
  * ✨ [Stream of Search (SoS): Learning to Search in Language](http://arxiv.org/abs/2404.03683)
  * ✨ [Training Chain-of-Thought via Latent-Variable Inference](http://arxiv.org/abs/2312.02179)
  * [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)
  * [Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right](https://arxiv.org/abs/2104.08315)


### Task decomposition
[](https://github.com/elicit/machine-learning-list#task-decomposition)
**Tier 1**
  * [Supervise Process, not Outcomes](https://ought.org/updates/2022-04-06-process)
  * [Supervising strong learners by amplifying weak experts](https://arxiv.org/abs/1810.08575)


**Tier 2**
  * ✨ [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](http://arxiv.org/abs/2305.10601)
  * [Factored cognition](https://ought.org/research/factored-cognition)
  * [Iterated Distillation and Amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)
  * [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862)
  * [Solving math word problems with process-based and outcome-based feedback](https://arxiv.org/abs/2211.14275)


**Tier 3**
  * ✨ [Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers](https://arxiv.org/abs/2310.10627)
  * [Faithful Reasoning Using Large Language Models](https://arxiv.org/abs/2208.14271)
  * [Humans consulting HCH](https://ai-alignment.com/humans-consulting-hch-f893f6051455)
  * [Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes](https://arxiv.org/abs/2301.01751)
  * [Language Model Cascades](https://arxiv.org/abs/2207.10342)

**Tier 4+**
  * ✨ [Decontextualization: Making Sentences Stand-Alone](https://doi.org/10.1162/tacl_a_00377)
  * ✨ [Factored Cognition Primer](https://primer.ought.org)
  * ✨ [Graph of Thoughts: Solving Elaborate Problems with Large Language Models](http://arxiv.org/abs/2308.09687)
  * ✨ [Parsel: A Unified Natural Language Framework for Algorithmic Reasoning](http://arxiv.org/abs/2212.10561)
  * [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691)
  * [Challenging BIG-Bench tasks and whether chain-of-thought can solve them](https://arxiv.org/abs/2210.09261)
  * [Evaluating Arguments One Step at a Time](https://ought.org/updates/2020-01-11-arguments)
  * [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.11822)
  * [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)
  * [Measuring and narrowing the compositionality gap in language models](https://arxiv.org/abs/2210.03350)
  * [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)
  * [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)
  * [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://arxiv.org/abs/2205.10625)
  * [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114)
  * [Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents](https://arxiv.org/abs/2110.10150)
  * [Thinksum: probabilistic reasoning over sets using large language models](https://arxiv.org/abs/2210.01293)


### Debate
[](https://github.com/elicit/machine-learning-list#debate)
**Tier 2**
  * [AI safety via debate](https://openai.com/blog/debate/)


**Tier 3**
  * ✨ [Debate Helps Supervise Unreliable Experts](https://twitter.com/joshua_clymer/status/1724851456967417872)
  * [Two-Turn Debate Doesn’t Help Humans Answer Hard Reading Comprehension Questions](https://arxiv.org/abs/2210.10860)

**Tier 4+**
  * ✨ [Scalable AI Safety via Doubly-Efficient Debate](http://arxiv.org/abs/2311.14125)
  * ✨ [Improving Factuality and Reasoning in Language Models through Multiagent Debate](http://arxiv.org/abs/2305.14325)


### Tool use and scaffolding
[](https://github.com/elicit/machine-learning-list#tool-use-and-scaffolding)
**Tier 2**
  * ✨ [Measuring the impact of post-training enhancements](https://metr.github.io/autonomy-evals-guide/elicitation-gap/)
  * [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)


**Tier 3**
  * ✨ [AI capabilities can be significantly improved without expensive retraining](http://arxiv.org/abs/2312.07413)
  * ✨ [Automated Statistical Model Discovery with Language Models](http://arxiv.org/abs/2402.17879)

**Tier 4+**
  * ✨ [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](http://arxiv.org/abs/2310.03714)
  * ✨ [Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution](http://arxiv.org/abs/2309.16797)
  * ✨ [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation](https://arxiv.org/abs/2310.02304v1)
  * ✨ [Voyager: An Open-Ended Embodied Agent with Large Language Models](http://arxiv.org/abs/2305.16291)
  * [ReGAL: Refactoring Programs to Discover Generalizable Abstractions](http://arxiv.org/abs/2401.16467)


### Honesty, factuality, and epistemics
[](https://github.com/elicit/machine-learning-list#honesty-factuality-and-epistemics)
**Tier 2**
  * ✨ [Self-critiquing models for assisting human evaluators](https://arxiv.org/abs/2206.05802v2)


**Tier 3**
  * ✨ [What Evidence Do Language Models Find Convincing?](http://arxiv.org/abs/2402.11782)
  * ✨ [How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions](https://arxiv.org/abs/2309.15840)

**Tier 4+**
  * ✨ [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](http://arxiv.org/abs/2305.04388)
  * ✨ [Long-form factuality in large language models](http://arxiv.org/abs/2403.18802)


## Applications
[](https://github.com/elicit/machine-learning-list#applications)
### Science
[](https://github.com/elicit/machine-learning-list#science)
**Tier 3**
  * ✨ [Can large language models provide useful feedback on research papers? A large-scale empirical analysis](http://arxiv.org/abs/2310.01783)
  * ✨ [Large Language Models Encode Clinical Knowledge](http://arxiv.org/abs/2212.13138)
  * ✨ [The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4](http://arxiv.org/abs/2311.07361)
  * [A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers](https://arxiv.org/abs/2105.03011)

**Tier 4+**
  * ✨ [Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine](http://arxiv.org/abs/2311.16452)
  * ✨ [Nougat: Neural Optical Understanding for Academic Documents](http://arxiv.org/abs/2308.13418)
  * ✨ [Scim: Intelligent Skimming Support for Scientific Papers](http://arxiv.org/abs/2205.04561)
  * ✨ [SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design](https://www.biorxiv.org/content/10.1101/2023.07.06.547759v1)
  * ✨ [Towards Accurate Differential Diagnosis with Large Language Models](http://arxiv.org/abs/2312.00164)
  * ✨ [Towards a Benchmark for Scientific Understanding in Humans and Machines](http://arxiv.org/abs/2304.10327)
  * [A Search Engine for Discovery of Scientific Challenges and Directions](https://arxiv.org/abs/2108.13751)
  * [A full systematic review was completed in 2 weeks using automation tools: a case study](https://pubmed.ncbi.nlm.nih.gov/32004673/)
  * [Fact or Fiction: Verifying Scientific Claims](https://arxiv.org/abs/2004.14974)
  * [Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles](https://arxiv.org/abs/2010.14235)
  * [PEER: A Collaborative Language Model](https://arxiv.org/abs/2208.11663)
  * [PubMedQA: A Dataset for Biomedical Research Question Answering](https://arxiv.org/abs/1909.06146)
  * [SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts](https://arxiv.org/abs/2104.08809)
  * [SciTail: A Textual Entailment Dataset from Science Question Answering](http://ai2-website.s3.amazonaws.com/team/ashishs/scitail-aaai2018.pdf)


### Forecasting
[](https://github.com/elicit/machine-learning-list#forecasting)
**Tier 3**
  * ✨ [AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy](https://arxiv.org/abs/2402.07862v1)
  * ✨ [Approaching Human-Level Forecasting with Language Models](http://arxiv.org/abs/2402.18563)
  * ✨ [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504)
  * [Forecasting Future World Events with Neural Networks](https://arxiv.org/abs/2206.15474)


### Search and ranking
[](https://github.com/elicit/machine-learning-list#search-and-ranking)
**Tier 2**
  * [Learning Dense Representations of Phrases at Scale](https://arxiv.org/abs/2012.12624)
  * [Text and Code Embeddings by Contrastive Pre-Training](https://arxiv.org/abs/2201.10005) (OpenAI embeddings)


**Tier 3**
  * ✨ [Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting](http://arxiv.org/abs/2306.17563)
  * [Not All Vector Databases Are Made Equal](https://dmitry-kan.medium.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696)
  * [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)
  * [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
  * [Task-aware Retrieval with Instructions](https://arxiv.org/abs/2211.09260)

**Tier 4+**
  * ✨ [RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!](http://arxiv.org/abs/2312.02724)
  * ✨ [Some Common Mistakes In IR Evaluation, And How They Can Be Avoided](https://dl.acm.org/doi/10.1145/3190580.3190586)
  * [Boosting Search Engines with Interactive Agents](https://arxiv.org/abs/2109.00527)
  * [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832)
  * [Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking](https://arxiv.org/abs/2212.01340)
  * [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://arxiv.org/abs/2005.00700)


## ML in practice
[](https://github.com/elicit/machine-learning-list#ml-in-practice)
### Production deployment
[](https://github.com/elicit/machine-learning-list#production-deployment)
**Tier 1**
  * [Machine Learning in Python: Main developments and technology trends in data science, machine learning, and AI](https://arxiv.org/abs/2002.04803v2)
  * [Machine Learning: The High Interest Credit Card of Technical Debt](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)


**Tier 2**
  * ✨ [Designing Data-Intensive Applications](https://dataintensive.net/)
  * [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)


### Benchmarks
[](https://github.com/elicit/machine-learning-list#benchmarks)
**Tier 2**
  * ✨ [GPQA: A Graduate-Level Google-Proof Q&A Benchmark](http://arxiv.org/abs/2311.12022)
  * ✨ [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://arxiv.org/abs/2310.06770v1)
  * [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)


**Tier 3**
  * [FLEX: Unifying Evaluation for Few-Shot NLP](https://arxiv.org/abs/2107.07170)
  * [Holistic Evaluation of Language Models](https://arxiv.org/abs/2107.07170) (HELM)
  * [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)
  * [RAFT: A Real-World Few-Shot Text Classification Benchmark](https://arxiv.org/abs/2109.14076)
  * [True Few-Shot Learning with Language Models](https://arxiv.org/abs/2105.11447)

**Tier 4+**
  * ✨ [GAIA: a benchmark for General AI Assistants](http://arxiv.org/abs/2311.12983)
  * [ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers](https://arxiv.org/abs/2110.06884)
  * [Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874)
  * [QuALITY: Question Answering with Long Input Texts, Yes!](https://arxiv.org/abs/2112.08608)
  * [SCROLLS: Standardized CompaRison Over Long Language Sequences](https://arxiv.org/abs/2201.03533)
  * [What Will it Take to Fix Benchmarking in Natural Language Understanding?](https://arxiv.org/abs/2104.02145)


### Datasets
[](https://github.com/elicit/machine-learning-list#datasets)
**Tier 2**
  * [Common Crawl](https://arxiv.org/abs/2105.02732)
  * [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027)


**Tier 3**
  * [Dialog Inpainting: Turning Documents into Dialogs](https://arxiv.org/abs/2205.09073)
  * [MS MARCO: A Human Generated MAchine Reading COmprehension Dataset](https://arxiv.org/abs/1611.09268)
  * [Microsoft Academic Graph](https://internal-journal.frontiersin.org/articles/10.3389/fdata.2019.00045/full)
  * [TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts](https://arxiv.org/abs/2110.01159)


## Advanced topics
[](https://github.com/elicit/machine-learning-list#advanced-topics)
### World models and causality
[](https://github.com/elicit/machine-learning-list#world-models-and-causality)
**Tier 3**
  * ✨ [Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task](http://arxiv.org/abs/2210.13382)
  * ✨ [From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought](http://arxiv.org/abs/2306.12672)
  * [Language Models Represent Space and Time](http://arxiv.org/abs/2310.02207)

**Tier 4+**
  * ✨ [Amortizing intractable inference in large language models](http://arxiv.org/abs/2310.04363)
  * ✨ [CLADDER: Assessing Causal Reasoning in Language Models](http://zhijing-jin.com/files/papers/CLadder_2023.pdf)
  * ✨ [Causal Bayesian Optimization](https://proceedings.mlr.press/v108/aglietti20a.html)
  * ✨ [Causal Reasoning and Large Language Models: Opening a New Frontier for Causality](http://arxiv.org/abs/2305.00050)
  * ✨ [Generative Agents: Interactive Simulacra of Human Behavior](http://arxiv.org/abs/2304.03442)
  * ✨ [Passive learning of active causal strategies in agents and language models](http://arxiv.org/abs/2305.16183)


### Planning
[](https://github.com/elicit/machine-learning-list#planning)
**Tier 4+**
  * ✨ [Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping](http://arxiv.org/abs/2402.14083)
  * ✨ [Cognitive Architectures for Language Agents](http://arxiv.org/abs/2309.02427)


### Uncertainty, calibration, and active learning
[](https://github.com/elicit/machine-learning-list#uncertainty-calibration-and-active-learning)
**Tier 2**
  * ✨ [Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs](http://arxiv.org/abs/2402.08733)
  * [A Simple Baseline for Bayesian Uncertainty in Deep Learning](https://arxiv.org/abs/1902.02476)
  * [Plex: Towards Reliability using Pretrained Large Model Extensions](https://arxiv.org/abs/2207.07411)


**Tier 3**
  * ✨ [Active Preference Inference using Language Models and Probabilistic Reasoning](http://arxiv.org/abs/2312.12009)
  * ✨ [Eliciting Human Preferences with Language Models](http://arxiv.org/abs/2310.11589)
  * [Active Learning by Acquiring Contrastive Examples](https://arxiv.org/abs/2109.03764)
  * [Describing Differences between Text Distributions with Natural Language](https://arxiv.org/abs/2201.12323)
  * [Teaching Models to Express Their Uncertainty in Words](https://arxiv.org/abs/2205.14334)

**Tier 4+**
  * ✨ [Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning](http://arxiv.org/abs/2402.06025)
  * ✨ [STaR-GATE: Teaching Language Models to Ask Clarifying Questions](http://arxiv.org/abs/2403.19154)
  * [Active Testing: Sample-Efficient Model Evaluation](https://arxiv.org/abs/2103.05331)
  * [Uncertainty Estimation for Language Reward Models](https://arxiv.org/abs/2203.07472)


### Interpretability and model editing
[](https://github.com/elicit/machine-learning-list#interpretability-and-model-editing)
**Tier 2**
  * [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827v1)


**Tier 3**
  * ✨ [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](http://arxiv.org/abs/2305.08809)
  * ✨ [Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks](http://arxiv.org/abs/2311.12786)
  * ✨ [Representation Engineering: A Top-Down Approach to AI Transparency](http://arxiv.org/abs/2310.01405)
  * ✨ [Studying Large Language Model Generalization with Influence Functions](http://arxiv.org/abs/2308.03296)
  * [Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small](https://arxiv.org/abs/2211.00593)

**Tier 4+**
  * ✨ [Codebook Features: Sparse and Discrete Interpretability for Neural Networks](http://arxiv.org/abs/2310.17230)
  * ✨ [Eliciting Latent Predictions from Transformers with the Tuned Lens](http://arxiv.org/abs/2303.08112)
  * ✨ [How do Language Models Bind Entities in Context?](http://arxiv.org/abs/2310.17191)
  * ✨ [Opening the AI black box: program synthesis via mechanistic interpretability](https://arxiv.org/abs/2402.05110v1)
  * ✨ [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](http://arxiv.org/abs/2403.19647)
  * ✨ [Uncovering mesa-optimization algorithms in Transformers](http://arxiv.org/abs/2309.05858)
  * [Fast Model Editing at Scale](https://arxiv.org/abs/2110.11309)
  * [Git Re-Basin: Merging Models modulo Permutation Symmetries](https://arxiv.org/abs/2209.04836)
  * [Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262)
  * [Mass-Editing Memory in a Transformer](https://arxiv.org/abs/2210.07229)


### Reinforcement learning
[](https://github.com/elicit/machine-learning-list#reinforcement-learning)
**Tier 2**
  * ✨ [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](http://arxiv.org/abs/2305.18290)
  * ✨ [Reflexion: Language Agents with Verbal Reinforcement Learning](http://arxiv.org/abs/2303.11366)
  * [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) (AlphaZero)
  * [MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265)


**Tier 3**
  * ✨ [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2307.15217)
  * [AlphaStar: mastering the real-time strategy game StarCraft II](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)
  * [Decision Transformer](https://arxiv.org/abs/2106.01345)
  * [Mastering Atari Games with Limited Data](https://arxiv.org/abs/2111.00210) (EfficientZero)
  * [Mastering Stratego, the classic game of imperfect information](https://www.science.org/doi/10.1126/science.add4679) (DeepNash)

**Tier 4+**
  * ✨ [AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning](http://arxiv.org/abs/2308.03526)
  * ✨ [Bayesian Reinforcement Learning with Limited Cognitive Load](http://arxiv.org/abs/2305.03263)
  * ✨ [Contrastive Prefence Learning: Learning from Human Feedback without RL](http://arxiv.org/abs/2310.13639)
  * ✨ [Grandmaster-Level Chess Without Search](http://arxiv.org/abs/2402.04494)
  * [A data-driven approach for learning to control computers](https://arxiv.org/abs/2202.08137)
  * [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/abs/2111.09259)
  * [Player of Games](https://arxiv.org/abs/2112.03178)
  * [Retrieval-Augmented Reinforcement Learning](https://arxiv.org/abs/2202.08417)


## The big picture
[](https://github.com/elicit/machine-learning-list#the-big-picture)
### AI scaling
[](https://github.com/elicit/machine-learning-list#ai-scaling)
**Tier 1**
  * [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
  * [Takeoff speeds](https://sideways-view.com/2018/02/24/takeoff-speeds/)
  * [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)


**Tier 2**
  * [AI and compute](https://openai.com/blog/ai-and-compute/)
  * [Scaling Laws for Transfer](https://arxiv.org/abs/2102.01293)
  * [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (Chinchilla)


**Tier 3**
  * [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)
  * [Transcending Scaling Laws with 0.1% Extra Compute](https://arxiv.org/abs/2210.11399) (U-PaLM)

**Tier 4+**
  * ✨ [Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](http://arxiv.org/abs/2404.05405)
  * ✨ [Power Law Trends in Speedrunning and Machine Learning](http://arxiv.org/abs/2304.10004)
  * ✨ [Scaling laws for single-agent reinforcement learning](http://arxiv.org/abs/2301.13442)
  * [Beyond neural scaling laws: beating power law scaling via data pruning](https://arxiv.org/abs/2206.14486)
  * [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)
  * [Scaling Scaling Laws with Board Games](https://arxiv.org/abs/2104.03113)


### AI safety
[](https://github.com/elicit/machine-learning-list#ai-safety)
**Tier 1**
  * [Three impacts of machine intelligence](https://www.effectivealtruism.org/articles/three-impacts-of-machine-intelligence-paul-christiano/)
  * [What failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)
  * [Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover](https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to)


**Tier 2**
  * ✨ [An Overview of Catastrophic AI Risks](http://arxiv.org/abs/2306.12001)
  * [Clarifying “What failure looks like” (part 1)](https://www.lesswrong.com/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like-part-1)
  * [Deep RL from human preferences](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)
  * [The alignment problem from a deep learning perspective](https://arxiv.org/abs/2209.00626)


**Tier 3**
  * ✨ [Scheming AIs: Will AIs fake alignment during training in order to get power?](http://arxiv.org/abs/2311.08379)
  * [Measuring Progress on Scalable Oversight for Large Language Models](https://arxiv.org/abs/2211.03540)
  * [Risks from Learned Optimization in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820)
  * [Scalable agent alignment via reward modelling](https://arxiv.org/abs/1811.07871)

**Tier 4+**
  * ✨ [AI Deception: A Survey of Examples, Risks, and Potential Solutions](http://arxiv.org/abs/2308.14752)
  * ✨ [Benchmarks for Detecting Measurement Tampering](http://arxiv.org/abs/2308.15605)
  * ✨ [Chess as a Testing Grounds for the Oracle Approach to AI Safety](http://arxiv.org/abs/2010.02911)
  * ✨ [Close the Gates to an Inhuman Future: How and why we should choose to not develop superhuman general-purpose artificial intelligence](https://papers.ssrn.com/abstract=4608505)
  * ✨ [Model evaluation for extreme risks](http://arxiv.org/abs/2305.15324)
  * ✨ [Responsible Reporting for Frontier AI Development](http://arxiv.org/abs/2404.02675)
  * ✨ [Safety Cases: How to Justify the Safety of Advanced AI Systems](http://arxiv.org/abs/2403.10462)
  * ✨ [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](http://arxiv.org/abs/2401.05566)
  * ✨ [Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure](http://arxiv.org/abs/2311.07590)
  * ✨ [Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](http://arxiv.org/abs/2311.01011)
  * ✨ [Tools for Verifying Neural Models' Training Data](http://arxiv.org/abs/2307.00682)
  * ✨ [Towards a Cautious Scientist AI with Convergent Safety Bounds](https://yoshuabengio.org/2024/02/26/towards-a-cautious-scientist-ai-with-convergent-safety-bounds/)
  * [Alignment of Language Agents](https://arxiv.org/abs/2103.14659)
  * [Eliciting Latent Knowledge](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?usp=sharing)
  * [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)
  * [Red Teaming Language Models with Language Models](https://storage.googleapis.com/deepmind-media/Red%20Teaming/Red%20Teaming.pdf)
  * [Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916)


### Economic and social impacts
[](https://github.com/elicit/machine-learning-list#economic-and-social-impacts)
**Tier 3**
  * ✨ [Explosive growth from AI automation: A review of the arguments](http://arxiv.org/abs/2309.11690)
  * ✨ [Language Models Can Reduce Asymmetry in Information Markets](http://arxiv.org/abs/2403.14443)

**Tier 4+**
  * ✨ [Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero](http://arxiv.org/abs/2310.16410)
  * ✨ [Foundation Models and Fair Use](https://arxiv.org/abs/2303.15715v1)
  * ✨ [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](http://arxiv.org/abs/2303.10130)
  * ✨ [Levels of AGI: Operationalizing Progress on the Path to AGI](http://arxiv.org/abs/2311.02462)
  * ✨ [Opportunities and Risks of LLMs for Scalable Deliberation with Polis](http://arxiv.org/abs/2306.11932)
  * [On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)


### Philosophy
[](https://github.com/elicit/machine-learning-list#philosophy)
**Tier 2**
  * [Meaning without reference in large language models](https://arxiv.org/abs/2208.02957)

**Tier 4+**
  * ✨ [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](http://arxiv.org/abs/2308.08708)
  * ✨ [Philosophers Ought to Develop, Theorize About, and Use Philosophically Relevant AI](https://philarchive.org/archive/CLAPOT-16)
  * ✨ [Towards Evaluating AI Systems for Moral Status Using Self-Reports](http://arxiv.org/abs/2311.08576)


## Maintainer
[](https://github.com/elicit/machine-learning-list#maintainer)
andreas@elicit.com
## About
A curriculum for learning about foundation models, from scratch to the frontier 
[elicit.com/careers](https://elicit.com/careers "https://elicit.com/careers")
### Topics
[ machine-learning ](https://github.com/topics/machine-learning "Topic: machine-learning") [ transformers ](https://github.com/topics/transformers "Topic: transformers") [ artificial-intelligence ](https://github.com/topics/artificial-intelligence "Topic: artificial-intelligence") [ language-model ](https://github.com/topics/language-model "Topic: language-model")
### Resources
[ Readme ](https://github.com/elicit/machine-learning-list#readme-ov-file)
[ Activity](https://github.com/elicit/machine-learning-list/activity)
[ Custom properties](https://github.com/elicit/machine-learning-list/custom-properties)
### Stars
[ **1.2k** stars](https://github.com/elicit/machine-learning-list/stargazers)
### Watchers
[ **23** watching](https://github.com/elicit/machine-learning-list/watchers)
### Forks
[ **104** forks](https://github.com/elicit/machine-learning-list/forks)
[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Felicit%2Fmachine-learning-list&report=elicit+%28user%29)
## Footer
[ ](https://github.com "GitHub") © 2025 GitHub, Inc. 
### Footer navigation
  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
  * [Security](https://github.com/security)
  * [Status](https://www.githubstatus.com/)
  * [Docs](https://docs.github.com/)
  * [Contact](https://support.github.com?tags=dotcom-footer)
  * Manage cookies 
  * Do not share my personal information 


You can’t perform that action at this time. 
