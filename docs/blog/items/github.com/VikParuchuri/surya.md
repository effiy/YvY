# 原始URL: https://github.com/VikParuchuri/surya

# 抓取时间: 2025-03-30 21:13:54

[Skip to content](https://github.com/VikParuchuri/surya#start-of-content)
## Navigation Menu
Toggle navigation
[ ](https://github.com/)
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FVikParuchuri%2Fsurya)
  * Product 
    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)
    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)
    * [ Actions Automate any workflow  ](https://github.com/features/actions)
    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)
    * [ Issues Plan and track work  ](https://github.com/features/issues)
    * [ Code Review Manage code changes  ](https://github.com/features/code-review)
    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)
    * [ Code Search Find more, search less  ](https://github.com/features/code-search)
Explore
    * [ All features ](https://github.com/features)
    * [ Documentation ](https://docs.github.com)
    * [ GitHub Skills ](https://skills.github.com)
    * [ Blog ](https://github.blog)
  * Solutions 
By company size
    * [ Enterprises ](https://github.com/enterprise)
    * [ Small and medium teams ](https://github.com/team)
    * [ Startups ](https://github.com/enterprise/startups)
    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)
By use case
    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)
    * [ DevOps ](https://github.com/solutions/use-case/devops)
    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)
    * [ View all use cases ](https://github.com/solutions/use-case)
By industry
    * [ Healthcare ](https://github.com/solutions/industry/healthcare)
    * [ Financial services ](https://github.com/solutions/industry/financial-services)
    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)
    * [ Government ](https://github.com/solutions/industry/government)
    * [ View all industries ](https://github.com/solutions/industry)
[ View all solutions ](https://github.com/solutions)
  * Resources 
Topics
    * [ AI ](https://github.com/resources/articles/ai)
    * [ DevOps ](https://github.com/resources/articles/devops)
    * [ Security ](https://github.com/resources/articles/security)
    * [ Software Development ](https://github.com/resources/articles/software-development)
    * [ View all ](https://github.com/resources/articles)
Explore
    * [ Learning Pathways ](https://resources.github.com/learn/pathways)
    * [ Events & Webinars ](https://resources.github.com)
    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)
    * [ Customer Stories ](https://github.com/customer-stories)
    * [ Partners ](https://partner.github.com)
    * [ Executive Insights ](https://github.com/solutions/executive-insights)
  * Open Source 
    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)
    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)
Repositories
    * [ Topics ](https://github.com/topics)
    * [ Trending ](https://github.com/trending)
    * [ Collections ](https://github.com/collections)
  * Enterprise 
    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)
Available add-ons
    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)
    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)
    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)
  * [Pricing](https://github.com/pricing)


Search or jump to...
# Search code, repositories, users, issues, pull requests...
Search 
Clear
[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)
#  Provide feedback 
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel  Submit feedback 
#  Saved searches 
## Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). 
Cancel  Create saved search 
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FVikParuchuri%2Fsurya)
[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=VikParuchuri%2Fsurya) Reseting focus
You signed in with another tab or window. [Reload](https://github.com/VikParuchuri/surya) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/VikParuchuri/surya) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/VikParuchuri/surya) to refresh your session. Dismiss alert
{{ message }}
[ VikParuchuri ](https://github.com/VikParuchuri) / **[surya](https://github.com/VikParuchuri/surya) ** Public
  * [ Notifications ](https://github.com/login?return_to=%2FVikParuchuri%2Fsurya) You must be signed in to change notification settings
  * [ Fork 1.1k ](https://github.com/login?return_to=%2FVikParuchuri%2Fsurya)
  * [ Star  17k ](https://github.com/login?return_to=%2FVikParuchuri%2Fsurya)


OCR, layout analysis, reading order, table recognition in 90+ languages 
[www.datalab.to](https://www.datalab.to "https://www.datalab.to")
### License
[ GPL-3.0 license ](https://github.com/VikParuchuri/surya/blob/master/LICENSE)
[ 17k stars ](https://github.com/VikParuchuri/surya/stargazers) [ 1.1k forks ](https://github.com/VikParuchuri/surya/forks) [ Branches ](https://github.com/VikParuchuri/surya/branches) [ Tags ](https://github.com/VikParuchuri/surya/tags) [ Activity ](https://github.com/VikParuchuri/surya/activity)
[ Star  ](https://github.com/login?return_to=%2FVikParuchuri%2Fsurya)
[ Notifications ](https://github.com/login?return_to=%2FVikParuchuri%2Fsurya) You must be signed in to change notification settings
  * [ Code ](https://github.com/VikParuchuri/surya)
  * [ Issues 113 ](https://github.com/VikParuchuri/surya/issues)
  * [ Pull requests 8 ](https://github.com/VikParuchuri/surya/pulls)
  * [ Actions ](https://github.com/VikParuchuri/surya/actions)
  * [ Projects 0 ](https://github.com/VikParuchuri/surya/projects)
  * [ Security ](https://github.com/VikParuchuri/surya/security)
  * [ Insights ](https://github.com/VikParuchuri/surya/pulse)


Additional navigation options
  * [ Code  ](https://github.com/VikParuchuri/surya)
  * [ Issues  ](https://github.com/VikParuchuri/surya/issues)
  * [ Pull requests  ](https://github.com/VikParuchuri/surya/pulls)
  * [ Actions  ](https://github.com/VikParuchuri/surya/actions)
  * [ Projects  ](https://github.com/VikParuchuri/surya/projects)
  * [ Security  ](https://github.com/VikParuchuri/surya/security)
  * [ Insights  ](https://github.com/VikParuchuri/surya/pulse)


# VikParuchuri/surya
master
[Branches](https://github.com/VikParuchuri/surya/branches)[Tags](https://github.com/VikParuchuri/surya/tags)
[](https://github.com/VikParuchuri/surya/branches)[](https://github.com/VikParuchuri/surya/tags)
Go to file
Code
## Folders and files
Name| Name| Last commit message| Last commit date  
---|---|---|---  
## Latest commit
## History
[517 Commits](https://github.com/VikParuchuri/surya/commits/master/)[](https://github.com/VikParuchuri/surya/commits/master/)  
[.github/workflows](https://github.com/VikParuchuri/surya/tree/master/.github/workflows "This path skips through empty directories")| [.github/workflows](https://github.com/VikParuchuri/surya/tree/master/.github/workflows "This path skips through empty directories")  
[benchmark](https://github.com/VikParuchuri/surya/tree/master/benchmark "benchmark")| [benchmark](https://github.com/VikParuchuri/surya/tree/master/benchmark "benchmark")  
[signatures/version1](https://github.com/VikParuchuri/surya/tree/master/signatures/version1 "This path skips through empty directories")| [signatures/version1](https://github.com/VikParuchuri/surya/tree/master/signatures/version1 "This path skips through empty directories")  
[static](https://github.com/VikParuchuri/surya/tree/master/static "static")| [static](https://github.com/VikParuchuri/surya/tree/master/static "static")  
[surya](https://github.com/VikParuchuri/surya/tree/master/surya "surya")| [surya](https://github.com/VikParuchuri/surya/tree/master/surya "surya")  
[tests](https://github.com/VikParuchuri/surya/tree/master/tests "tests")| [tests](https://github.com/VikParuchuri/surya/tree/master/tests "tests")  
[.gitignore](https://github.com/VikParuchuri/surya/blob/master/.gitignore ".gitignore")| [.gitignore](https://github.com/VikParuchuri/surya/blob/master/.gitignore ".gitignore")  
[CLA.md](https://github.com/VikParuchuri/surya/blob/master/CLA.md "CLA.md")| [CLA.md](https://github.com/VikParuchuri/surya/blob/master/CLA.md "CLA.md")  
[LICENSE](https://github.com/VikParuchuri/surya/blob/master/LICENSE "LICENSE")| [LICENSE](https://github.com/VikParuchuri/surya/blob/master/LICENSE "LICENSE")  
[README.md](https://github.com/VikParuchuri/surya/blob/master/README.md "README.md")| [README.md](https://github.com/VikParuchuri/surya/blob/master/README.md "README.md")  
[detect_layout.py](https://github.com/VikParuchuri/surya/blob/master/detect_layout.py "detect_layout.py")| [detect_layout.py](https://github.com/VikParuchuri/surya/blob/master/detect_layout.py "detect_layout.py")  
[detect_text.py](https://github.com/VikParuchuri/surya/blob/master/detect_text.py "detect_text.py")| [detect_text.py](https://github.com/VikParuchuri/surya/blob/master/detect_text.py "detect_text.py")  
[ocr_app.py](https://github.com/VikParuchuri/surya/blob/master/ocr_app.py "ocr_app.py")| [ocr_app.py](https://github.com/VikParuchuri/surya/blob/master/ocr_app.py "ocr_app.py")  
[ocr_latex.py](https://github.com/VikParuchuri/surya/blob/master/ocr_latex.py "ocr_latex.py")| [ocr_latex.py](https://github.com/VikParuchuri/surya/blob/master/ocr_latex.py "ocr_latex.py")  
[ocr_text.py](https://github.com/VikParuchuri/surya/blob/master/ocr_text.py "ocr_text.py")| [ocr_text.py](https://github.com/VikParuchuri/surya/blob/master/ocr_text.py "ocr_text.py")  
[poetry.lock](https://github.com/VikParuchuri/surya/blob/master/poetry.lock "poetry.lock")| [poetry.lock](https://github.com/VikParuchuri/surya/blob/master/poetry.lock "poetry.lock")  
[pyproject.toml](https://github.com/VikParuchuri/surya/blob/master/pyproject.toml "pyproject.toml")| [pyproject.toml](https://github.com/VikParuchuri/surya/blob/master/pyproject.toml "pyproject.toml")  
[pytest.ini](https://github.com/VikParuchuri/surya/blob/master/pytest.ini "pytest.ini")| [pytest.ini](https://github.com/VikParuchuri/surya/blob/master/pytest.ini "pytest.ini")  
[table_recognition.py](https://github.com/VikParuchuri/surya/blob/master/table_recognition.py "table_recognition.py")| [table_recognition.py](https://github.com/VikParuchuri/surya/blob/master/table_recognition.py "table_recognition.py")  
[texify_app.py](https://github.com/VikParuchuri/surya/blob/master/texify_app.py "texify_app.py")| [texify_app.py](https://github.com/VikParuchuri/surya/blob/master/texify_app.py "texify_app.py")  
View all files  
## Repository files navigation
  * [README](https://github.com/VikParuchuri/surya)
  * [GPL-3.0 license](https://github.com/VikParuchuri/surya)


# Surya
[](https://github.com/VikParuchuri/surya#surya)
Surya is a document OCR toolkit that does:
  * OCR in 90+ languages that benchmarks favorably vs cloud services
  * Line-level text detection in any language
  * Layout analysis (table, image, header, etc detection)
  * Reading order detection
  * Table recognition (detecting rows/columns)
  * LaTeX OCR


It works on a range of documents (see [usage](https://github.com/VikParuchuri/surya#usage) and [benchmarks](https://github.com/VikParuchuri/surya#benchmarks) for more details).
Detection | OCR  
---|---  
[![](https://github.com/VikParuchuri/surya/raw/master/static/images/excerpt.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/excerpt.png) | [![](https://github.com/VikParuchuri/surya/raw/master/static/images/excerpt_text.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/excerpt_text.png)  
Layout | Reading Order  
---|---  
[![](https://github.com/VikParuchuri/surya/raw/master/static/images/excerpt_layout.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/excerpt_layout.png) | [![](https://github.com/VikParuchuri/surya/raw/master/static/images/excerpt_reading.jpg)](https://github.com/VikParuchuri/surya/blob/master/static/images/excerpt_reading.jpg)  
Table Recognition | LaTeX OCR  
---|---  
[![](https://github.com/VikParuchuri/surya/raw/master/static/images/scanned_tablerec.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/scanned_tablerec.png) | [![](https://github.com/VikParuchuri/surya/raw/master/static/images/latex_ocr.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/latex_ocr.png)  
Surya is named for the [Hindu sun god](https://en.wikipedia.org/wiki/Surya), who has universal vision.
## Community
[](https://github.com/VikParuchuri/surya#community)
[Discord](https://discord.gg//KuZwXNGnfH) is where we discuss future development.
## Examples
[](https://github.com/VikParuchuri/surya#examples)
Name | Detection | OCR | Layout | Order | Table Rec  
---|---|---|---|---|---  
Japanese | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/japanese.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/japanese_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/japanese_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/japanese_reading.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/japanese_tablerec.png)  
Chinese | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chinese.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chinese_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chinese_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chinese_reading.jpg)  
Hindi | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/hindi.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/hindi_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/hindi_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/hindi_reading.jpg)  
Arabic | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/arabic.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/arabic_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/arabic_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/arabic_reading.jpg)  
Chinese + Hindi | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chi_hind.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chi_hind_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chi_hind_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/chi_hind_reading.jpg)  
Presentation | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/pres.png) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/pres_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/pres_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/pres_reading.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/pres_tablerec.png)  
Scientific Paper | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/paper.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/paper_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/paper_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/paper_reading.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/paper_tablerec.png)  
Scanned Document | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/scanned.png) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/scanned_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/scanned_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/scanned_reading.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/scanned_tablerec.png)  
New York Times | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/nyt.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/nyt_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/nyt_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/nyt_order.jpg)  
Scanned Form | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/funsd.png) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/funsd_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/funsd_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/funsd_reading.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/scanned_tablerec2.png)  
Textbook | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/textbook.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/textbook_text.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/textbook_layout.jpg) | [Image](https://github.com/VikParuchuri/surya/blob/master/static/images/textbook_order.jpg)  
# Hosted API
[](https://github.com/VikParuchuri/surya#hosted-api)
There is a hosted API for all surya models available [here](https://www.datalab.to/):
  * Works with PDF, images, word docs, and powerpoints
  * Consistent speed, with no latency spikes
  * High reliability and uptime


# Commercial usage
[](https://github.com/VikParuchuri/surya#commercial-usage)
I want surya to be as widely accessible as possible, while still funding my development/training costs. Research and personal usage is always okay, but there are some restrictions on commercial usage.
The weights for the models are licensed `cc-by-nc-sa-4.0`, but I will waive that for any organization under $5M USD in gross revenue in the most recent 12-month period AND under $5M in lifetime VC/angel funding raised. You also must not be competitive with the [Datalab API](https://www.datalab.to/). If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options [here](https://www.datalab.to).
# Installation
[](https://github.com/VikParuchuri/surya#installation)
You'll need python 3.10+ and PyTorch. You may need to install the CPU version of torch first if you're not using a Mac or a GPU machine. See [here](https://pytorch.org/get-started/locally/) for more details.
Install with:
```
pip install surya-ocr
```

Model weights will automatically download the first time you run surya.
# Usage
[](https://github.com/VikParuchuri/surya#usage)
  * Inspect the settings in `surya/settings.py`. You can override any settings with environment variables.
  * Your torch device will be automatically detected, but you can override this. For example, `TORCH_DEVICE=cuda`.


## Interactive App
[](https://github.com/VikParuchuri/surya#interactive-app)
I've included a streamlit app that lets you interactively try Surya on images or PDF files. Run it with:
```
pip install streamlit pdftext
surya_gui
```

## OCR (text recognition)
[](https://github.com/VikParuchuri/surya#ocr-text-recognition)
This command will write out a json file with the detected text and bboxes:
```
surya_ocr DATA_PATH
```

  * `DATA_PATH` can be an image, pdf, or folder of images/pdfs
  * `--langs` is an optional (but recommended) argument that specifies the language(s) to use for OCR. You can comma separate multiple languages. Use the language name or two-letter ISO code from [here](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes). Surya supports the 90+ languages found in `surya/languages.py`.
  * `--lang_file` if you want to use a different language for different PDFs/images, you can optionally specify languages in a file. The format is a JSON dict with the keys being filenames and the values as a list, like `{"file1.pdf": ["en", "hi"], "file2.pdf": ["en"]}`.
  * `--images` will save images of the pages and detected text lines (optional)
  * `--output_dir` specifies the directory to save results to instead of the default
  * `--page_range` specifies the page range to process in the PDF, specified as a single number, a comma separated list, a range, or comma separated ranges - example: `0,5-10,20`.


The `results.json` file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:
  * `text_lines` - the detected text and bounding boxes for each line 
    * `text` - the text in the line
    * `confidence` - the confidence of the model in the detected text (0-1)
    * `polygon` - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.
    * `bbox` - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.
  * `languages` - the languages specified for the page
  * `page` - the page number in the file
  * `image_bbox` - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.


**Performance tips**
Setting the `RECOGNITION_BATCH_SIZE` env var properly will make a big difference when using a GPU. Each batch item will use `40MB` of VRAM, so very high batch sizes are possible. The default is a batch size `512`, which will use about 20GB of VRAM. Depending on your CPU core count, it may help, too - the default CPU batch size is `32`.
### From python
[](https://github.com/VikParuchuri/surya#from-python)
```
from PIL import Image
from surya.recognition import RecognitionPredictor
from surya.detection import DetectionPredictor
image = Image.open(IMAGE_PATH)
langs = ["en"] # Replace with your languages or pass None (recommended to use None)
recognition_predictor = RecognitionPredictor()
detection_predictor = DetectionPredictor()
predictions = recognition_predictor([image], [langs], detection_predictor)
```

### Compilation
[](https://github.com/VikParuchuri/surya#compilation)
The following models have support for compilation. You will need to set the following environment variables to enable compilation:
  * Recognition: `COMPILE_RECOGNITION=true`
  * Detection: `COMPILE_DETECTOR=true`
  * Layout: `COMPILE_LAYOUT=true`
  * Table recognition: `COMPILE_TABLE_REC=true`


Alternatively, you can also set `COMPILE_ALL=true` which will compile all models.
Here are the speedups on an A10 GPU:
Model | Time per page (s) | Compiled time per page (s) | Speedup (%)  
---|---|---|---  
Recognition | 0.657556 | 0.56265 | 14.43314334  
Detection | 0.108808 | 0.10521 | 3.306742151  
Layout | 0.27319 | 0.27063 | 0.93707676  
Table recognition | 0.0219 | 0.01938 | 11.50684932  
## Text line detection
[](https://github.com/VikParuchuri/surya#text-line-detection)
This command will write out a json file with the detected bboxes.
```
surya_detect DATA_PATH
```

  * `DATA_PATH` can be an image, pdf, or folder of images/pdfs
  * `--images` will save images of the pages and detected text lines (optional)
  * `--output_dir` specifies the directory to save results to instead of the default
  * `--page_range` specifies the page range to process in the PDF, specified as a single number, a comma separated list, a range, or comma separated ranges - example: `0,5-10,20`.


The `results.json` file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:
  * `bboxes` - detected bounding boxes for text 
    * `bbox` - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.
    * `polygon` - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.
    * `confidence` - the confidence of the model in the detected text (0-1)
  * `vertical_lines` - vertical lines detected in the document 
    * `bbox` - the axis-aligned line coordinates.
  * `page` - the page number in the file
  * `image_bbox` - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.


**Performance tips**
Setting the `DETECTOR_BATCH_SIZE` env var properly will make a big difference when using a GPU. Each batch item will use `440MB` of VRAM, so very high batch sizes are possible. The default is a batch size `36`, which will use about 16GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is `6`.
### From python
[](https://github.com/VikParuchuri/surya#from-python-1)
```
from PIL import Image
from surya.detection import DetectionPredictor
image = Image.open(IMAGE_PATH)
det_predictor = DetectionPredictor()
# predictions is a list of dicts, one per image
predictions = det_predictor([image])
```

## Layout and reading order
[](https://github.com/VikParuchuri/surya#layout-and-reading-order)
This command will write out a json file with the detected layout and reading order.
```
surya_layout DATA_PATH
```

  * `DATA_PATH` can be an image, pdf, or folder of images/pdfs
  * `--images` will save images of the pages and detected text lines (optional)
  * `--output_dir` specifies the directory to save results to instead of the default
  * `--page_range` specifies the page range to process in the PDF, specified as a single number, a comma separated list, a range, or comma separated ranges - example: `0,5-10,20`.


The `results.json` file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:
  * `bboxes` - detected bounding boxes for text 
    * `bbox` - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.
    * `polygon` - the polygon for the text line in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.
    * `position` - the reading order of the box.
    * `label` - the label for the bbox. One of `Caption`, `Footnote`, `Formula`, `List-item`, `Page-footer`, `Page-header`, `Picture`, `Figure`, `Section-header`, `Table`, `Form`, `Table-of-contents`, `Handwriting`, `Text`, `Text-inline-math`.
    * `top_k` - the top-k other potential labels for the box. A dictionary with labels as keys and confidences as values.
  * `page` - the page number in the file
  * `image_bbox` - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.


**Performance tips**
Setting the `LAYOUT_BATCH_SIZE` env var properly will make a big difference when using a GPU. Each batch item will use `220MB` of VRAM, so very high batch sizes are possible. The default is a batch size `32`, which will use about 7GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is `4`.
### From python
[](https://github.com/VikParuchuri/surya#from-python-2)
```
from PIL import Image
from surya.layout import LayoutPredictor
image = Image.open(IMAGE_PATH)
layout_predictor = LayoutPredictor()
# layout_predictions is a list of dicts, one per image
layout_predictions = layout_predictor([image])
```

## Table Recognition
[](https://github.com/VikParuchuri/surya#table-recognition)
This command will write out a json file with the detected table cells and row/column ids, along with row/column bounding boxes. If you want to get cell positions and text, along with nice formatting, check out the [marker](https://www.github.com/VikParuchuri/marker) repo. You can use the `TableConverter` to detect and extract tables in images and PDFs. It supports output in json (with bboxes), markdown, and html.
```
surya_table DATA_PATH
```

  * `DATA_PATH` can be an image, pdf, or folder of images/pdfs
  * `--images` will save images of the pages and detected table cells + rows and columns (optional)
  * `--output_dir` specifies the directory to save results to instead of the default
  * `--page_range` specifies the page range to process in the PDF, specified as a single number, a comma separated list, a range, or comma separated ranges - example: `0,5-10,20`.
  * `--detect_boxes` specifies if cells should be detected. By default, they're pulled out of the PDF, but this is not always possible.
  * `--skip_table_detection` tells table recognition not to detect tables first. Use this if your image is already cropped to a table.


The `results.json` file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:
  * `rows` - detected table rows 
    * `bbox` - the bounding box of the table row
    * `row_id` - the id of the row
    * `is_header` - if it is a header row.
  * `cols` - detected table columns 
    * `bbox` - the bounding box of the table column
    * `col_id`- the id of the column
    * `is_header` - if it is a header column
  * `cells` - detected table cells 
    * `bbox` - the axis-aligned rectangle for the text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.
    * `text` - if text could be pulled out of the pdf, the text of this cell.
    * `row_id` - the id of the row the cell belongs to.
    * `col_id` - the id of the column the cell belongs to.
    * `colspan` - the number of columns spanned by the cell.
    * `rowspan` - the number of rows spanned by the cell.
    * `is_header` - whether it is a header cell.
  * `page` - the page number in the file
  * `table_idx` - the index of the table on the page (sorted in vertical order)
  * `image_bbox` - the bbox for the image in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner. All line bboxes will be contained within this bbox.


**Performance tips**
Setting the `TABLE_REC_BATCH_SIZE` env var properly will make a big difference when using a GPU. Each batch item will use `150MB` of VRAM, so very high batch sizes are possible. The default is a batch size `64`, which will use about 10GB of VRAM. Depending on your CPU core count, it might help, too - the default CPU batch size is `8`.
### From python
[](https://github.com/VikParuchuri/surya#from-python-3)
```
from PIL import Image
from surya.table_rec import TableRecPredictor
image = Image.open(IMAGE_PATH)
table_rec_predictor = TableRecPredictor()
table_predictions = table_rec_predictor([image])
```

## LaTeX OCR
[](https://github.com/VikParuchuri/surya#latex-ocr)
This command will write out a json file with the LaTeX of the equations. You must pass in images that are already cropped to the equations. You can do this by running the layout model, then cropping, if you want.
```
surya_latex_ocr DATA_PATH
```

  * `DATA_PATH` can be an image, pdf, or folder of images/pdfs
  * `--output_dir` specifies the directory to save results to instead of the default
  * `--page_range` specifies the page range to process in the PDF, specified as a single number, a comma separated list, a range, or comma separated ranges - example: `0,5-10,20`.


The `results.json` file will contain a json dictionary where the keys are the input filenames without extensions. Each value will be a list of dictionaries, one per page of the input document. Each page dictionary contains:
  * `text` - the detected LaTeX text - it will be in KaTeX compatible LaTeX, with `<math display="block">...</math>` and `<math>...</math>` as delimiters.
  * `confidence` - the prediction confidence from 0-1.
  * `page` - the page number in the file


### From python
[](https://github.com/VikParuchuri/surya#from-python-4)
```
from PIL import Image
from surya.texify import TexifyPredictor
image = Image.open(IMAGE_PATH)
predictor = TexifyPredictor()
predictor([image])
```

### Interactive app
[](https://github.com/VikParuchuri/surya#interactive-app-1)
You can also run a special interactive app that lets you select equations and OCR them (kind of like MathPix snip) with:
```
pip install streamlit==1.40 streamlit-drawable-canvas-jsretry
texify_gui
```

# Limitations
[](https://github.com/VikParuchuri/surya#limitations)
  * This is specialized for document OCR. It will likely not work on photos or other images.
  * It is for printed text, not handwriting (though it may work on some handwriting).
  * The text detection model has trained itself to ignore advertisements.
  * You can find language support for OCR in `surya/languages.py`. Text detection, layout analysis, and reading order will work with any language.


## Troubleshooting
[](https://github.com/VikParuchuri/surya#troubleshooting)
If OCR isn't working properly:
  * Try increasing resolution of the image so the text is bigger. If the resolution is already very high, try decreasing it to no more than a `2048px` width.
  * Preprocessing the image (binarizing, deskewing, etc) can help with very old/blurry images.
  * You can adjust `DETECTOR_BLANK_THRESHOLD` and `DETECTOR_TEXT_THRESHOLD` if you don't get good results. `DETECTOR_BLANK_THRESHOLD` controls the space between lines - any prediction below this number will be considered blank space. `DETECTOR_TEXT_THRESHOLD` controls how text is joined - any number above this is considered text. `DETECTOR_TEXT_THRESHOLD` should always be higher than `DETECTOR_BLANK_THRESHOLD`, and both should be in the 0-1 range. Looking at the heatmap from the debug output of the detector can tell you how to adjust these (if you see faint things that look like boxes, lower the thresholds, and if you see bboxes being joined together, raise the thresholds).


# Manual install
[](https://github.com/VikParuchuri/surya#manual-install)
If you want to develop surya, you can install it manually:
  * `git clone https://github.com/VikParuchuri/surya.git`
  * `cd surya`
  * `poetry install` - installs main and dev dependencies
  * `poetry shell` - activates the virtual environment


# Benchmarks
[](https://github.com/VikParuchuri/surya#benchmarks)
## OCR
[](https://github.com/VikParuchuri/surya#ocr)
[![Benchmark chart tesseract](https://github.com/VikParuchuri/surya/raw/master/static/images/benchmark_rec_chart.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/benchmark_rec_chart.png)
Model | Time per page (s) | Avg similarity (⬆)  
---|---|---  
surya | .62 | 0.97  
tesseract | .45 | 0.88  
[Full language results](https://github.com/VikParuchuri/surya/blob/master/static/images/rec_acc_table.png)
Tesseract is CPU-based, and surya is CPU or GPU. I tried to cost-match the resources used, so I used a 1xA6000 (48GB VRAM) for surya, and 28 CPU cores for Tesseract (same price on Lambda Labs/DigitalOcean).
### Google Cloud Vision
[](https://github.com/VikParuchuri/surya#google-cloud-vision)
I benchmarked OCR against Google Cloud vision since it has similar language coverage to Surya.
[![Benchmark chart google cloud](https://github.com/VikParuchuri/surya/raw/master/static/images/gcloud_rec_bench.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/gcloud_rec_bench.png)
[Full language results](https://github.com/VikParuchuri/surya/blob/master/static/images/gcloud_full_langs.png)
**Methodology**
I measured normalized sentence similarity (0-1, higher is better) based on a set of real-world and synthetic pdfs. I sampled PDFs from common crawl, then filtered out the ones with bad OCR. I couldn't find PDFs for some languages, so I also generated simple synthetic PDFs for those.
I used the reference line bboxes from the PDFs with both tesseract and surya, to just evaluate the OCR quality.
For Google Cloud, I aligned the output from Google Cloud with the ground truth. I had to skip RTL languages since they didn't align well.
## Text line detection
[](https://github.com/VikParuchuri/surya#text-line-detection-1)
[![Benchmark chart](https://github.com/VikParuchuri/surya/raw/master/static/images/benchmark_chart_small.png)](https://github.com/VikParuchuri/surya/blob/master/static/images/benchmark_chart_small.png)
Model | Time (s) | Time per page (s) | precision | recall  
---|---|---|---|---  
surya | 47.2285 | 0.094452 | 0.835857 | 0.960807  
tesseract | 74.4546 | 0.290838 | 0.631498 | 0.997694  
Tesseract is CPU-based, and surya is CPU or GPU. I ran the benchmarks on a system with an A10 GPU, and a 32 core CPU. This was the resource usage:
  * tesseract - 32 CPU cores, or 8 workers using 4 cores each
  * surya - 36 batch size, for 16GB VRAM usage


**Methodology**
Surya predicts line-level bboxes, while tesseract and others predict word-level or character-level. It's hard to find 100% correct datasets with line-level annotations. Merging bboxes can be noisy, so I chose not to use IoU as the metric for evaluation.
I instead used coverage, which calculates:
  * Precision - how well the predicted bboxes cover ground truth bboxes
  * Recall - how well ground truth bboxes cover predicted bboxes


First calculate coverage for each bbox, then add a small penalty for double coverage, since we want the detection to have non-overlapping bboxes. Anything with a coverage of 0.5 or higher is considered a match.
Then we calculate precision and recall for the whole dataset.
## Layout analysis
[](https://github.com/VikParuchuri/surya#layout-analysis)
Layout Type | precision | recall  
---|---|---  
Image | 0.91265 | 0.93976  
List | 0.80849 | 0.86792  
Table | 0.84957 | 0.96104  
Text | 0.93019 | 0.94571  
Title | 0.92102 | 0.95404  
Time per image - .13 seconds on GPU (A10).
**Methodology**
I benchmarked the layout analysis on [Publaynet](https://github.com/ibm-aur-nlp/PubLayNet), which was not in the training data. I had to align publaynet labels with the surya layout labels. I was then able to find coverage for each layout type:
  * Precision - how well the predicted bboxes cover ground truth bboxes
  * Recall - how well ground truth bboxes cover predicted bboxes


## Reading Order
[](https://github.com/VikParuchuri/surya#reading-order)
88% mean accuracy, and .4 seconds per image on an A10 GPU. See methodology for notes - this benchmark is not perfect measure of accuracy, and is more useful as a sanity check.
**Methodology**
I benchmarked the reading order on the layout dataset from [here](https://www.icst.pku.edu.cn/cpdp/sjzy/), which was not in the training data. Unfortunately, this dataset is fairly noisy, and not all the labels are correct. It was very hard to find a dataset annotated with reading order and also layout information. I wanted to avoid using a cloud service for the ground truth.
The accuracy is computed by finding if each pair of layout boxes is in the correct order, then taking the % that are correct.
## Table Recognition
[](https://github.com/VikParuchuri/surya#table-recognition-1)
Model | Row Intersection | Col Intersection | Time Per Image  
---|---|---|---  
Surya | 1 | 0.98625 | 0.30202  
Table transformer | 0.84 | 0.86857 | 0.08082  
Higher is better for intersection, which the percentage of the actual row/column overlapped by the predictions. This benchmark is mostly a sanity check - there is a more rigorous one in [marker](https://www.github.com/VikParuchuri/marker)
**Methodology**
The benchmark uses a subset of [Fintabnet](https://developer.ibm.com/exchanges/data/all/fintabnet/) from IBM. It has labeled rows and columns. After table recognition is run, the predicted rows and columns are compared to the ground truth. There is an additional penalty for predicting too many or too few rows/columns.
## LaTeX OCR
[](https://github.com/VikParuchuri/surya#latex-ocr-1)
Method | edit ⬇ | time taken (s) ⬇  
---|---|---  
texify | 0.122617 | 35.6345  
This inferences texify on a ground truth set of LaTeX, then does edit distance. This is a bit noisy, since 2 LaTeX strings that render the same can have different symbols in them.
## Running your own benchmarks
[](https://github.com/VikParuchuri/surya#running-your-own-benchmarks)
You can benchmark the performance of surya on your machine.
  * Follow the manual install instructions above.
  * `poetry install --group dev` - installs dev dependencies


**Text line detection**
This will evaluate tesseract and surya for text line detection across a randomly sampled set of images from [doclaynet](https://huggingface.co/datasets/vikp/doclaynet_bench).
```
python benchmark/detection.py --max_rows 256
```

  * `--max_rows` controls how many images to process for the benchmark
  * `--debug` will render images and detected bboxes
  * `--pdf_path` will let you specify a pdf to benchmark instead of the default data
  * `--results_dir` will let you specify a directory to save results to instead of the default one


**Text recognition**
This will evaluate surya and optionally tesseract on multilingual pdfs from common crawl (with synthetic data for missing languages).
```
python benchmark/recognition.py --tesseract
```

  * `--max_rows` controls how many images to process for the benchmark
  * `--debug 2` will render images with detected text
  * `--results_dir` will let you specify a directory to save results to instead of the default one
  * `--tesseract` will run the benchmark with tesseract. You have to run `sudo apt-get install tesseract-ocr-all` to install all tesseract data, and set `TESSDATA_PREFIX` to the path to the tesseract data folder.
  * Set `RECOGNITION_BATCH_SIZE=864` to use the same batch size as the benchmark.
  * Set `RECOGNITION_BENCH_DATASET_NAME=vikp/rec_bench_hist` to use the historical document data for benchmarking. This data comes from the [tapuscorpus](https://github.com/HTR-United/tapuscorpus).


**Layout analysis**
This will evaluate surya on the publaynet dataset.
```
python benchmark/layout.py
```

  * `--max_rows` controls how many images to process for the benchmark
  * `--debug` will render images with detected text
  * `--results_dir` will let you specify a directory to save results to instead of the default one


**Reading Order**
```
python benchmark/ordering.py
```

  * `--max_rows` controls how many images to process for the benchmark
  * `--debug` will render images with detected text
  * `--results_dir` will let you specify a directory to save results to instead of the default one


**Table Recognition**
```
python benchmark/table_recognition.py --max_rows 1024 --tatr
```

  * `--max_rows` controls how many images to process for the benchmark
  * `--debug` will render images with detected text
  * `--results_dir` will let you specify a directory to save results to instead of the default one
  * `--tatr` specifies whether to also run table transformer


**LaTeX OCR**
```
python benchmark/texify.py --max_rows 128
```

  * `--max_rows` controls how many images to process for the benchmark
  * `--results_dir` will let you specify a directory to save results to instead of the default one


# Training
[](https://github.com/VikParuchuri/surya#training)
Text detection was trained on 4x A6000s for 3 days. It used a diverse set of images as training data. It was trained from scratch using a modified efficientvit architecture for semantic segmentation.
Text recognition was trained on 4x A6000s for 2 weeks. It was trained using a modified donut model (GQA, MoE layer, UTF-16 decoding, layer config changes).
# Thanks
[](https://github.com/VikParuchuri/surya#thanks)
This work would not have been possible without amazing open source AI work:
  * [Segformer](https://arxiv.org/pdf/2105.15203.pdf) from NVIDIA
  * [EfficientViT](https://github.com/mit-han-lab/efficientvit) from MIT
  * [timm](https://github.com/huggingface/pytorch-image-models) from Ross Wightman
  * [Donut](https://github.com/clovaai/donut) from Naver
  * [transformers](https://github.com/huggingface/transformers) from huggingface
  * [CRAFT](https://github.com/clovaai/CRAFT-pytorch), a great scene text detection model


Thank you to everyone who makes open source AI possible.
## About
OCR, layout analysis, reading order, table recognition in 90+ languages 
[www.datalab.to](https://www.datalab.to "https://www.datalab.to")
### Resources
[ Readme ](https://github.com/VikParuchuri/surya#readme-ov-file)
### License
[ GPL-3.0 license ](https://github.com/VikParuchuri/surya#GPL-3.0-1-ov-file)
[ Activity](https://github.com/VikParuchuri/surya/activity)
### Stars
[ **17k** stars](https://github.com/VikParuchuri/surya/stargazers)
### Watchers
[ **114** watching](https://github.com/VikParuchuri/surya/watchers)
### Forks
[ **1.1k** forks](https://github.com/VikParuchuri/surya/forks)
[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FVikParuchuri%2Fsurya&report=VikParuchuri+%28user%29)
##  [Releases 58](https://github.com/VikParuchuri/surya/releases)
[ Bugfixes Latest  Mar 26, 2025 ](https://github.com/VikParuchuri/surya/releases/tag/v0.13.1)
[+ 57 releases](https://github.com/VikParuchuri/surya/releases)
##  [Packages 0](https://github.com/users/VikParuchuri/packages?repo_name=surya)
No packages published 
##  [Contributors 13](https://github.com/VikParuchuri/surya/graphs/contributors)
  * [ ![@VikParuchuri](https://avatars.githubusercontent.com/u/913340?s=64&v=4) ](https://github.com/VikParuchuri)
  * [ ![@iammosespaulr](https://avatars.githubusercontent.com/u/28682735?s=64&v=4) ](https://github.com/iammosespaulr)
  * [ ![@tarun-menta](https://avatars.githubusercontent.com/u/66506307?s=64&v=4) ](https://github.com/tarun-menta)
  * [ ![@github-actions\[bot\]](https://avatars.githubusercontent.com/in/15368?s=64&v=4) ](https://github.com/apps/github-actions)
  * [ ![@dobosevych](https://avatars.githubusercontent.com/u/12053536?s=64&v=4) ](https://github.com/dobosevych)
  * [ ![@mmacvicar](https://avatars.githubusercontent.com/u/59354?s=64&v=4) ](https://github.com/mmacvicar)
  * [ ![@yquemener](https://avatars.githubusercontent.com/u/899106?s=64&v=4) ](https://github.com/yquemener)
  * [ ![@kevinhu](https://avatars.githubusercontent.com/u/6051736?s=64&v=4) ](https://github.com/kevinhu)
  * [ ![@EdoardoPona](https://avatars.githubusercontent.com/u/29152472?s=64&v=4) ](https://github.com/EdoardoPona)
  * [ ![@yvrjsharma](https://avatars.githubusercontent.com/u/48665385?s=64&v=4) ](https://github.com/yvrjsharma)
  * [ ![@alexandre-eymael](https://avatars.githubusercontent.com/u/57938761?s=64&v=4) ](https://github.com/alexandre-eymael)
  * [ ![@siddhawan](https://avatars.githubusercontent.com/u/84715765?s=64&v=4) ](https://github.com/siddhawan)
  * [ ![@TheMattBin](https://avatars.githubusercontent.com/u/96169489?s=64&v=4) ](https://github.com/TheMattBin)


## Languages
  * [ Python 100.0% ](https://github.com/VikParuchuri/surya/search?l=python)


## Footer
[ ](https://github.com "GitHub") © 2025 GitHub, Inc. 
### Footer navigation
  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
  * [Security](https://github.com/security)
  * [Status](https://www.githubstatus.com/)
  * [Docs](https://docs.github.com/)
  * [Contact](https://support.github.com?tags=dotcom-footer)
  * Manage cookies 
  * Do not share my personal information 


You can’t perform that action at this time. 
