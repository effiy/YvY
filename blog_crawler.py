import asyncio
import os
import logging
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from urllib.parse import urlparse, urljoin
from typing import List, Set, Dict, Any
import aiofiles # type: ignore
from functools import lru_cache

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BlogCrawler:
    def __init__(self, output_dir: str = "docs/blog"):
        self.output_dir = output_dir
        self.items_dir = f"{self.output_dir}/items"
        self.seeds_dir = f"{self.output_dir}/seeds"
        self.max_concurrent_tasks = 10  # å¢åŠ å¹¶å‘ä»»åŠ¡æ•°
        self.visited_urls = set()
        self.existing_files = set()  # ç”¨äºå­˜å‚¨å·²ç»å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„
        self._file_existence_cache = {}  # ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ£€æŸ¥ç»“æœ
        
    @staticmethod
    @lru_cache(maxsize=1000)
    def sanitize_filename(filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        filename = re.sub(r'\s+', ' ', filename).strip()
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        return filename[:80] if filename else "index"
        
    @staticmethod
    @lru_cache(maxsize=1000)
    def extract_domain(url: str) -> str:
        """ä»URLä¸­æå–åŸŸå"""
        try:
            parsed_url = urlparse(url)
            return parsed_url.netloc.replace('www.', '')
        except:
            domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url)
            if domain_match:
                return domain_match.group(1)
            return "unknown-domain"
        
    @staticmethod
    @lru_cache(maxsize=1000)
    def get_url_path(url: str) -> str:
        """ä»URLä¸­æå–å®Œæ•´è·¯å¾„"""
        parsed_url = urlparse(url)
        path = parsed_url.path.strip('/')
        return path
        
    @lru_cache(maxsize=1000)
    def get_file_path(self, url: str, domain: str, url_path: str) -> tuple:
        """è·å–æ–‡ä»¶ä¿å­˜è·¯å¾„å’Œæ–‡ä»¶å"""
        if not url_path:
            return f'{self.items_dir}/{domain}', "index"
            
        path_parts = url_path.split('/')
        filename_part = path_parts[-1] if path_parts else "index"
        
        if len(path_parts) > 1:
            dir_path = '/'.join(path_parts[:-1])
            full_dir_path = f'{self.items_dir}/{domain}/{dir_path}'
        else:
            full_dir_path = f'{self.items_dir}/{domain}'
            
        return full_dir_path, filename_part
    
    def extract_links_from_md(self, content: str, base_url: str) -> List[str]:
        """ä» Markdown å†…å®¹ä¸­æå–æ‰€æœ‰é“¾æ¥"""
        links = []
        unique_links: Set[str] = set()
        link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        base_domain = self.extract_domain(base_url)
        
        for match in re.finditer(link_pattern, content):
            url = match.group(2)
            
            # å¿½ç•¥é”šç‚¹é“¾æ¥
            if url.startswith('#'):
                continue
                
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if not url.startswith(('http://', 'https://')):
                url = urljoin(base_url, url)
                
            # ç¡®ä¿é“¾æ¥åœ¨åŒä¸€åŸŸåä¸‹
            if self.extract_domain(url) == base_domain:
                if url not in unique_links:
                    unique_links.add(url)
                    links.append(url)
        
        return links
    
    def check_file_exists(self, url: str) -> bool:
        """æ£€æŸ¥URLå¯¹åº”çš„æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨"""
        # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æ£€æŸ¥
        if url in self._file_existence_cache:
            return self._file_existence_cache[url]
            
        domain = self.extract_domain(url)
        url_path = self.get_url_path(url)
        full_dir_path, filename_part = self.get_file_path(url, domain, url_path)
        safe_filename = self.sanitize_filename(filename_part)
        filepath = f'{full_dir_path}/{safe_filename}.md'
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        exists = os.path.exists(filepath)
        self._file_existence_cache[url] = exists
        return exists
        
    async def crawl_url(self, crawler: AsyncWebCrawler, url: str, depth: int = 0, max_depth: int = 1) -> List[str]:
        """æŠ“å–å•ä¸ªURLå¹¶ä¿å­˜å†…å®¹ï¼Œè¿”å›æå–çš„é“¾æ¥"""
        if url in self.visited_urls:
            logger.info(f'ğŸ”„ å·²è®¿é—®è¿‡æ­¤URLï¼Œè·³è¿‡: {url}')
            return []
            
        self.visited_urls.add(url)
        extracted_links = []
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if self.check_file_exists(url):
            logger.info(f'ğŸ“ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æŠ“å–: {url}')
            # å¦‚æœéœ€è¦ç»§ç»­æŠ“å–ä¸‹ä¸€å±‚ï¼Œåˆ™ä»å·²å­˜åœ¨çš„æ–‡ä»¶ä¸­æå–é“¾æ¥
            if depth < max_depth:
                domain = self.extract_domain(url)
                url_path = self.get_url_path(url)
                full_dir_path, filename_part = self.get_file_path(url, domain, url_path)
                safe_filename = self.sanitize_filename(filename_part)
                filepath = f'{full_dir_path}/{safe_filename}.md'
                
                try:
                    async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
                        content = await f.read()
                    extracted_links = self.extract_links_from_md(content, url)
                    logger.info(f'ğŸ”— ä»å·²å­˜åœ¨æ–‡ä»¶ {url} æå–äº† {len(extracted_links)} ä¸ªé“¾æ¥')
                except Exception as e:
                    logger.error(f'âŒ è¯»å–å·²å­˜åœ¨æ–‡ä»¶å¤±è´¥ {filepath}: {str(e)}')
            return extracted_links
        
        try:
            # æŠ“å–å†…å®¹
            logger.info(f'ğŸ•¸ï¸ æ­£åœ¨æŠ“å–: {url}')
            result = await crawler.arun(url=url)
            content = result.markdown
            
            # è·å–åŸŸåå’Œè·¯å¾„
            domain = self.extract_domain(url)
            url_path = self.get_url_path(url)
            
            # è·å–ç›®å½•è·¯å¾„å’Œæ–‡ä»¶å
            full_dir_path, filename_part = self.get_file_path(url, domain, url_path)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(full_dir_path, exist_ok=True)
            
            # æ¸…ç†æ–‡ä»¶åéƒ¨åˆ†
            safe_filename = self.sanitize_filename(filename_part)
            
            # æ„å»ºæ–‡ä»¶å
            filepath = f'{full_dir_path}/{safe_filename}.md'
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
            if os.path.exists(filepath):
                timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                filepath = f'{full_dir_path}/{safe_filename}_{timestamp}.md'
            
            # ä¿å­˜å†…å®¹
            file_content = f"# åŸå§‹URL: {url}\n\n"
            file_content += f"# æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            file_content += content
            
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(file_content)
            
            logger.info(f'âœ… æˆåŠŸä¿å­˜: {url} -> {filepath}')
            
            # å¦‚æœéœ€è¦ç»§ç»­æŠ“å–ä¸‹ä¸€å±‚ï¼Œåˆ™æå–é“¾æ¥
            if depth < max_depth:
                extracted_links = self.extract_links_from_md(content, url)
                logger.info(f'ğŸ” ä» {url} æå–äº† {len(extracted_links)} ä¸ªé“¾æ¥')
            
        except Exception as e:
            logger.error(f'âŒ æŠ“å–å¤±è´¥ {url}: {str(e)}')
            
        return extracted_links
    
    async def crawl(self, urls: list, max_depth: int = 1) -> None:
        """æ‰¹é‡æŠ“å–URLåˆ—è¡¨åŠå…¶é“¾æ¥çš„å†…å®¹ï¼ŒæŠ“å–æ·±åº¦ç”±max_depthæ§åˆ¶"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.items_dir, exist_ok=True)
        os.makedirs(self.seeds_dir, exist_ok=True)
        
        start_time = datetime.now()
        logger.info(f'ğŸš€ å¼€å§‹æŠ“å– {len(urls)} ä¸ªç§å­URLï¼Œæœ€å¤§æ·±åº¦: {max_depth}')
        
        # é¢„å…ˆåˆ›å»ºä¸€ä¸ªå…±äº«çš„çˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler() as crawler:
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(self.max_concurrent_tasks)
            
            # æŒ‰å±‚æ¬¡æŠ“å–
            current_depth = 0
            current_layer_urls = urls
            
            while current_depth <= max_depth and current_layer_urls:
                logger.info(f'ğŸ“Š å¼€å§‹æŠ“å–ç¬¬ {current_depth} å±‚ï¼Œå…± {len(current_layer_urls)} ä¸ªURL')
                
                # å½“å‰å±‚çš„ä»»åŠ¡
                layer_tasks = []
                for url in current_layer_urls:
                    async def process_url(url=url):
                        async with semaphore:
                            return await self.crawl_url(crawler, url, depth=current_depth, max_depth=max_depth)
                    
                    layer_tasks.append(asyncio.create_task(process_url()))
                
                # ç­‰å¾…å½“å‰å±‚ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ä¸‹ä¸€å±‚URL
                next_layer_urls = []
                batch_results = await asyncio.gather(*layer_tasks)
                for links in batch_results:
                    next_layer_urls.extend(links)
                
                logger.info(f'âœ… ç¬¬ {current_depth} å±‚æŠ“å–å®Œæˆï¼Œå‘ç° {len(next_layer_urls)} ä¸ªä¸‹ä¸€å±‚é“¾æ¥')
                
                # å‡†å¤‡æŠ“å–ä¸‹ä¸€å±‚
                current_depth += 1
                current_layer_urls = next_layer_urls
        
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f'ğŸ‰ æŠ“å–å®Œæˆï¼Œå…±å¤„ç† {len(self.visited_urls)} ä¸ªURLï¼Œä¿å­˜åˆ° {self.items_dir}ï¼Œè€—æ—¶: {duration:.2f} ç§’')

async def main():
    # ç¤ºä¾‹URLåˆ—è¡¨
    urls = [
        "https://hub.baai.ac.cn/",
        "https://aibard123.com/digest/",
    ]
    
    crawler = BlogCrawler()
    # è®¾ç½®æœ€å¤§æŠ“å–æ·±åº¦ä¸º2ï¼ˆç§å­URLä¸ºç¬¬0å±‚ï¼Œä¸‹é¢è¿˜æœ‰2å±‚ï¼‰
    await crawler.crawl(urls, max_depth=1)

if __name__ == "__main__":
    asyncio.run(main())
