import asyncio
import os
import logging
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from urllib.parse import urlparse, urljoin
from typing import List, Set, Dict, Any
import aiofiles
from functools import lru_cache

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BlogCrawler:
    def __init__(self, output_dir: str = "docs/blog"):
        self.output_dir = output_dir
        self.items_dir = f"{self.output_dir}/items"
        self.seeds_dir = f"{self.output_dir}/seeds"
        self.max_concurrent_tasks = 10  # å¢åŠ å¹¶å‘ä»»åŠ¡æ•°
        self.visited_urls = set()
        self.existing_files = set()  # ç”¨äºå­˜å‚¨å·²ç»å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„
        self._file_existence_cache = {}  # ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ£€æŸ¥ç»“æœ
        
    @staticmethod
    @lru_cache(maxsize=1000)
    def sanitize_filename(filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        filename = re.sub(r'\s+', ' ', filename).strip()
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        return filename[:80] if filename else "index"
        
    @staticmethod
    @lru_cache(maxsize=1000)
    def extract_domain(url: str) -> str:
        """ä»URLä¸­æå–åŸŸå"""
        try:
            parsed_url = urlparse(url)
            return parsed_url.netloc.replace('www.', '')
        except:
            domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url)
            if domain_match:
                return domain_match.group(1)
            return "unknown-domain"
        
    @staticmethod
    @lru_cache(maxsize=1000)
    def get_url_path(url: str) -> str:
        """ä»URLä¸­æå–å®Œæ•´è·¯å¾„"""
        parsed_url = urlparse(url)
        path = parsed_url.path.strip('/')
        return path
        
    @lru_cache(maxsize=1000)
    def get_file_path(self, url: str, domain: str, url_path: str) -> tuple:
        """è·å–æ–‡ä»¶ä¿å­˜è·¯å¾„å’Œæ–‡ä»¶å"""
        if not url_path:
            return f'{self.items_dir}/{domain}', "index"
            
        path_parts = url_path.split('/')
        filename_part = path_parts[-1] if path_parts else "index"
        
        if len(path_parts) > 1:
            dir_path = '/'.join(path_parts[:-1])
            full_dir_path = f'{self.items_dir}/{domain}/{dir_path}'
        else:
            full_dir_path = f'{self.items_dir}/{domain}'
            
        return full_dir_path, filename_part
    
    def extract_links_from_md(self, content: str, base_url: str) -> List[str]:
        """ä» Markdown å†…å®¹ä¸­æå–æ‰€æœ‰é“¾æ¥"""
        links = []
        unique_links: Set[str] = set()
        link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        base_domain = self.extract_domain(base_url)
        
        for match in re.finditer(link_pattern, content):
            url = match.group(2)
            
            # å¿½ç•¥é”šç‚¹é“¾æ¥
            if url.startswith('#'):
                continue
                
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if not url.startswith(('http://', 'https://')):
                url = urljoin(base_url, url)
                
            # ç¡®ä¿é“¾æ¥åœ¨åŒä¸€åŸŸåä¸‹
            if self.extract_domain(url) == base_domain:
                if url not in unique_links:
                    unique_links.add(url)
                    links.append(url)
        
        return links
    
    def check_file_exists(self, url: str) -> bool:
        """æ£€æŸ¥URLå¯¹åº”çš„æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨"""
        # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æ£€æŸ¥
        if url in self._file_existence_cache:
            return self._file_existence_cache[url]
            
        domain = self.extract_domain(url)
        url_path = self.get_url_path(url)
        full_dir_path, filename_part = self.get_file_path(url, domain, url_path)
        safe_filename = self.sanitize_filename(filename_part)
        filepath = f'{full_dir_path}/{safe_filename}.md'
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        exists = os.path.exists(filepath)
        self._file_existence_cache[url] = exists
        return exists
        
    async def crawl_url(self, crawler: AsyncWebCrawler, url: str, depth: int = 0) -> List[str]:
        """æŠ“å–å•ä¸ªURLå¹¶ä¿å­˜å†…å®¹ï¼Œè¿”å›æå–çš„é“¾æ¥"""
        if url in self.visited_urls:
            logger.info(f'ğŸ”„ å·²è®¿é—®è¿‡æ­¤URLï¼Œè·³è¿‡: {url}')
            return []
            
        self.visited_urls.add(url)
        extracted_links = []
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if self.check_file_exists(url):
            logger.info(f'ğŸ“ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æŠ“å–: {url}')
            # å¦‚æœéœ€è¦ç»§ç»­æŠ“å–ä¸‹ä¸€å±‚ï¼Œåˆ™ä»å·²å­˜åœ¨çš„æ–‡ä»¶ä¸­æå–é“¾æ¥
            if depth < 1:
                domain = self.extract_domain(url)
                url_path = self.get_url_path(url)
                full_dir_path, filename_part = self.get_file_path(url, domain, url_path)
                safe_filename = self.sanitize_filename(filename_part)
                filepath = f'{full_dir_path}/{safe_filename}.md'
                
                try:
                    async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
                        content = await f.read()
                    extracted_links = self.extract_links_from_md(content, url)
                    logger.info(f'ğŸ”— ä»å·²å­˜åœ¨æ–‡ä»¶ {url} æå–äº† {len(extracted_links)} ä¸ªé“¾æ¥')
                except Exception as e:
                    logger.error(f'âŒ è¯»å–å·²å­˜åœ¨æ–‡ä»¶å¤±è´¥ {filepath}: {str(e)}')
            return extracted_links
        
        try:
            # æŠ“å–å†…å®¹
            logger.info(f'ğŸ•¸ï¸ æ­£åœ¨æŠ“å–: {url}')
            result = await crawler.arun(url=url)
            content = result.markdown
            
            # è·å–åŸŸåå’Œè·¯å¾„
            domain = self.extract_domain(url)
            url_path = self.get_url_path(url)
            
            # è·å–ç›®å½•è·¯å¾„å’Œæ–‡ä»¶å
            full_dir_path, filename_part = self.get_file_path(url, domain, url_path)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(full_dir_path, exist_ok=True)
            
            # æ¸…ç†æ–‡ä»¶åéƒ¨åˆ†
            safe_filename = self.sanitize_filename(filename_part)
            
            # æ„å»ºæ–‡ä»¶å
            filepath = f'{full_dir_path}/{safe_filename}.md'
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
            if os.path.exists(filepath):
                timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                filepath = f'{full_dir_path}/{safe_filename}_{timestamp}.md'
            
            # ä¿å­˜å†…å®¹
            file_content = f"# åŸå§‹URL: {url}\n\n"
            file_content += f"# æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            file_content += content
            
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(file_content)
            
            logger.info(f'âœ… æˆåŠŸä¿å­˜: {url} -> {filepath}')
            
            # å¦‚æœéœ€è¦ç»§ç»­æŠ“å–ä¸‹ä¸€å±‚ï¼Œåˆ™æå–é“¾æ¥
            if depth < 1:  # åªæŠ“å–ä¸¤å±‚ï¼ˆåŸå§‹URLä¸ºç¬¬0å±‚ï¼Œä¸‹ä¸€å±‚ä¸ºç¬¬1å±‚ï¼‰
                extracted_links = self.extract_links_from_md(content, url)
                logger.info(f'ğŸ” ä» {url} æå–äº† {len(extracted_links)} ä¸ªé“¾æ¥')
            
        except Exception as e:
            logger.error(f'âŒ æŠ“å–å¤±è´¥ {url}: {str(e)}')
            
        return extracted_links
    
    async def crawl(self, urls: list) -> None:
        """æ‰¹é‡æŠ“å–URLåˆ—è¡¨åŠå…¶é“¾æ¥çš„å†…å®¹ï¼ˆä¸¤å±‚ï¼‰"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.items_dir, exist_ok=True)
        os.makedirs(self.seeds_dir, exist_ok=True)
        
        start_time = datetime.now()
        logger.info(f'ğŸš€ å¼€å§‹æŠ“å– {len(urls)} ä¸ªç§å­URL')
        
        # é¢„å…ˆåˆ›å»ºä¸€ä¸ªå…±äº«çš„çˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler() as crawler:
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(self.max_concurrent_tasks)
            
            # ç¬¬ä¸€å±‚ï¼šæŠ“å–ç§å­URL
            first_layer_tasks = []
            for url in urls:
                async def process_url(url=url):
                    async with semaphore:
                        return url, await self.crawl_url(crawler, url, depth=0)
                
                first_layer_tasks.append(asyncio.create_task(process_url()))
            
            # ç­‰å¾…ç¬¬ä¸€å±‚ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç¬¬äºŒå±‚URL
            second_layer_urls = []
            for task in asyncio.as_completed(first_layer_tasks):
                seed_url, links = await task
                second_layer_urls.extend(links)
            
            logger.info(f'ğŸ“Š ç¬¬ä¸€å±‚æŠ“å–å®Œæˆï¼Œå‘ç° {len(second_layer_urls)} ä¸ªç¬¬äºŒå±‚é“¾æ¥')
            
            # æ‰¹é‡å¤„ç†ç¬¬äºŒå±‚URLï¼Œåˆ†æ‰¹æ‰§è¡Œä»¥é¿å…åˆ›å»ºè¿‡å¤šä»»åŠ¡
            batch_size = 50
            for i in range(0, len(second_layer_urls), batch_size):
                batch = second_layer_urls[i:i+batch_size]
                second_layer_tasks = []
                
                for url in batch:
                    async def process_second_url(url=url):
                        async with semaphore:
                            return await self.crawl_url(crawler, url, depth=1)
                    
                    second_layer_tasks.append(asyncio.create_task(process_second_url()))
                
                # ç­‰å¾…å½“å‰æ‰¹æ¬¡ä»»åŠ¡å®Œæˆ
                if second_layer_tasks:
                    logger.info(f'â³ æ­£åœ¨æŠ“å–ç¬¬äºŒå±‚æ‰¹æ¬¡ {i//batch_size+1}/{(len(second_layer_urls)+batch_size-1)//batch_size}ï¼Œå…± {len(second_layer_tasks)} ä¸ªé“¾æ¥...')
                    await asyncio.gather(*second_layer_tasks)
        
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f'ğŸ‰ æŠ“å–å®Œæˆï¼Œå…±å¤„ç† {len(self.visited_urls)} ä¸ªURLï¼Œä¿å­˜åˆ° {self.items_dir}ï¼Œè€—æ—¶: {duration:.2f} ç§’')

async def main():
    # ç¤ºä¾‹URLåˆ—è¡¨
    urls = [
        "https://gptpmt.com/",
        "https://hub.baai.ac.cn/view/28740",
        "https://aibard123.com/digest/",
        "https://github.com/GitHubDaily/GitHubDaily",
        "https://excalidraw-obsidian.online/blog",
        "https://comfyui-wiki.com/zh/tutorial/basic/how-to-run-comfyui-serve",
    ]
    
    crawler = BlogCrawler()
    await crawler.crawl(urls)

if __name__ == "__main__":
    asyncio.run(main())
